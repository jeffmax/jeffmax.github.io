{"pages":[{"text":"Most of developers have had to generate a public/private key pair at one point or another. Often this is to create a certificate for a webserver, but it comes up for various reasons. Whenever I have to do this I am always extremely confused and amazed when it all works out, so in this post I am going to try to explain what is actually going on. Everything here will assume we are using RSA, which is complicated enough for one day. Caveat: I am not an expert (or even close) in cryptography. If anything is wrong in this article please use the comments section to correct me. I am very aware that wrong information in this topic is often much worse than no information at all. The Basics Using the openssl command you can do tons of stuff, so much stuff that sometimes it is very unclear what you are actually doing. Lets start with as basic as you can get. Generate an RSA public/private key pair Private Key As expained here openssl genrsa - out privatekey . pem 1024 This creates a file called privatekey.pem. This is actually kind of confusing already, because that file actually contains both your private and public key. What is this .pem file This .pem file holds your private/public keys. It is in the PEM encoding, which is a human readable version of the DER encoding (see below). If you open one, typically you will see some kind of header like -----BEGIN RSA PRIVATE KEY----- followed by a series of characters and then a footer. One file can contain a series of objects like this. The mess of characters is a base64 encoded of ASN.1. As PEM is encoding as well as used a file extension, you will sometimes see a .key or .crt or .pub file that is in PEM format. A .pem extension itself doesn't really tell us much about what is in it. A PEM encoded file can contain a private/public key pair, just a public key or an entire certificate. The one we just generated just contains a private/public key pair There are also DER files that can contain the same information except in binary format and without the headers and footers. I have run into .der files because iOS expects them. This link goes over some of the confusion points here. The openssl command can convert back and forth between this formats. DER encoding seems to be used more frequently (or perhaps all the time?) to hold certificates rather than just keys. Some .crt files are in DER format. Public Key So now we have this .pem file with a private and public key, but if you want to do anything useful with this stuff you will need to be able to share only your public key. So the following extracts your public key openssl rsa - in mykey . pem - pubout > mykey . pub This is a .pub file, but again, it's really just a PEM encoded file with only your public key. I believe the formats for these files are specified here Do something useful So at this point, you have come to a cross road. What can we do with these things? We can generate a certificate for some task. We use them directly them for some larger crypto scheme like digital signatures. Certificates Certificates are useful if you are trying to setup a server for https or just need to work within the confines of a system that works best with certificates. For example, if you wanted to use your public key on iOS, the primitive functions exposed there expect a certificate even if you don't need any kind of chain of ownership. To generate a certificate you use openssl to create a Certificate Signing Request. In our case, we have already have a private/public key pair, so you would the following command openssl req - out CSR . csr - key mykey . key - new You would then either submit this to an organization that would sign it for you, or sign it yourself to create a self-signed certificate. openssl x509 - req - days XXX in CSR . csr - signkey privatekey . pem - our server . crt Ultimately if this was your goal, you could have done everything up to now (except extract the public key) in one step: openssl req - x509 - newkey rsa : 2048 - keyout privatekey . pem - out cert . crt - days XXX See this link for a breakdown of this command and how to not require a password. iOS By the way, as mentioned briefly above, if you want to do anything involving a public/private key on iOS, you are going to want a self signed certificate in DER form. This link explains how to do this from scratch or from the .pem file we created in the first step. Keep in mind that you have a few options on iOS, but if you don't want a solution that involves bundling openssl with your app, you need the self signed der file so you can use Apple's builtin functions. If you just want to verify a signed cert, I will go over that below. Technically to use a public key on iOS you basically need to get your key into a SecKeyRef. There are various ways to do this, see here and here and this pod . If you just need to verify a signature, look at the first link. This link doesn't show how to load the certifcate, but shows both signing and verifying. You may need to tweak the functions and constants referring to SHA1 if you want to use SHA256. You can always read the Apple docs for more concrete information. Verify a signature on iOS Coming soon.. Use keys directly Without a certificate (or with one) you can still create digital signatures for documents and verify them as well as encrypt and decrypt documents. You just use your private key to sign and other people use your public key to verify the signature (or someone else encrypts a document with your public key and you decrypt it with your private key). You can do this in Python using the M2Crypto, PyCrypt or rsa libraries. One thing to keep in mind- with RSA there is usually a \"textbook\" conceptual way to do things that involves the pure math, and then the safe, blessed implementation schemes. Something that really bit me once was that PyCrypt implements both a \"textbook\" version of RSA signatures and the two official signature schemes (RSASSA-PSS and RSASSA-PKCS1-v1_5). Apparently the \"textbook\" one is not safe at all. See my comment here . The difference between the \"textbook\" version and the two official schemes has to do with extra padding (or encoding) done between hashing and signing that prevents certain types of attacks. You can see technical details of the RSASSA-PCKS1-v1_5 here . One interesting thing to note there is that scheme hashes your document with a particular hash function (that you can specify) and then encodes which hash function was used as part of the padding before it actually signs. This is why depending on the api you might need to hash your document first and then pass a symbol indicating which hash function you used to the sign or verify method. RSASSA-PKCS1-v1_5 is newer and more complex- but ultimately either is safer than naively hashing your document (with a SHA family function) and encrypting it with your RSA key (which is what I thought I could do after reading about digital signatures).","tags":"certificates","loc":"http://jeffmax.io/developer-introduction-to-rsa-keys-and-ssl-certificates.html","title":"Developer introduction to RSA keys and SSL certificates"},{"text":"I am currently enrolled in an applied statistics program. For my course I really want to use the IPython Notebook. It allows me to do the calculations and show formulas all in one document. I have been trying figure out how to use Python instead of a statistics program. The functionality is all there, but is spread across different libraries, and there are many choices when it comes to graphing. In this post I will show to do some simple statistics (mostly related to linear regression) using Python. Requirements numpy scipy pandas statsmodels matplotlib seaborn Getting data into Python Data is typically in some column-based format (either in Excel, or a tab/comma delimeted file). CSV is the easiest to import, so use Excel to export whatever you have into CSV. Then Pandas has a method to quickly get data from a CSV file into Python (into a Pandas DataFrame object). Pandas provides a lot of functionality, but at its core is the DataFrame and Series object. from pandas import DataFrame data = DataFrame . from_csv ( \"filename.csv\" , index_col = None )) The DataFrame is made up of Series objects (one for each column of your data). You can easily get to a Series by indexing into the DataFrame. data [ \"Column1\" ] The columns are basically Numpy arrays- you can easily apply an operation to every value in the column. Simple scatter plot One of the frustrating things about getting up to speed with performing statistics in Python is that there is often more than one way to accomplish something. For example, Pandas allows you to easily create a scatter plot of two variables, but you can also do this directly through Matplotlib. In general, in my limited experience, if you can avoid dropping down to matplotlib, then do so. People have spent a lot of time worrying about the details so you don't need to. Unfortunately, it is likely you will need to learn it at some point to get exactly what you want. With pandas data . plot ( x = \"Column1\" , y = \"Columns2\" , kind = \"scatter\" ) Matplotlib column1 = data [ \"Column1\" ] column2 = data [ \"Column2\" ] fig = plt . figure () axes = fig . add_axes ([ 0 , 0 , . 8 , . 8 ]) axes . scatter ( column1 , column2 , marker = \"o\" ) There are quite a few ways to accomplish the same thing in matplotlib. This is a decent place to get started: http://matplotlib.org/1.3.1/users/pyplot_tutorial.html, although it uses the Matlab style interface which I find gets confusing very fast. The alternative (and the one I used above) is the object oriented mode where you instantiate individual figure objects and operate on them instead of using a implicit global figure object. This link does a good job showing examples of both styles: http://nbviewer.ipython.org/github/jrjohansson/scientific-python-lectures/blob/master/Lecture-4-Matplotlib.ipynb. A tip- seaborn, which is a library for statistics visualization that uses matplotlib, will set some visual defaults for matplotlib as soon as you import it. Try importing and regenerating one of the graphs above to see the difference. import seabon as sns Simple Linear Regression Another example, you can do simple linear regression using the statsmodels library, but Pandas wraps that functionality up making it a little easier to perform when you are working with a DataFrame object. Directly through Pandas DataFrame from pandas.stats.api import ols ols ( x = data [ \"Column1\" ], y = data [ \"Column2])) Statsmodels import statsmodels.api as sm X = sm . add_constant ( data [ \"Column1\" ]) model = sm . OLS ( data [ \"Column2\" ], X ) model . fit () . summary () You can also use the statsmodel formula api to do the same thing model = ols ( 'Column2 ~ Column1' , data = data ) Residuals vs Fits Generating a graph of Residuals versus the fitted y values has come up a few times. I searched all over for a library taht does this out of the box, but could not find it. Seaborn comes close with import seaborn as sns sns . residplot ( \"Column1\" , \"Column2\" , data = data ) But unforutnately this creates a plot of residuals vs the x values. I think in many situations, this graph will tell you the same thing as residuals versis fitted values, but it is not what I was looking for. Here is how to create it with vanilla matplotlib fitted = 10 + 0.7 * data [ \"Column1\" ] # This just needs to be whatever the linear regrsssion equation is fig = plt . figure () axes = fig . add_axes ([ 0 , 0 , 1.0 , 1 , 0 ]) axes . axhline ( color = \"black\" , ls = \"--\" ) # This creates a horizon line (like abline in R) axes . set_xlabel ( 'Fitted Value' ) axes . set_ylabel ( 'Residuals' ) axes . set_title ( 'Residuals Versus Fits' ) axes . scatter ( fitted , data [ \"Column2\" ] - fitted , marker = \"o\" ) ANOVA for linear regression Not much to say here. from statsmodels.stats.anova import anova_lm from statsmodels.formula.api import ols model = ols ( 'Column2 ~ Column1' , data = data ) anova_lm ( model . fit ())","tags":"Python","loc":"http://jeffmax.io/using-python-for-statistics-coursework.html","title":"Using Python for Statistics Coursework"},{"text":"When you are writing an iOS app and have beta testers you need a method to update their devices with your latest release. Running around with a laptop and a cable will get old very fast. There are a few commercial solutions that I am sure work well, but this method is free and fairly painless. This post is a recapitulation of several posts already available on the web (see http://readwrite.com/2010/12/15/apple-best-kept-secret-how-to-do-ad-hoc-installs, http://www.paradeofrain.com/2010/11/taking-the-pain-out-of-ad-hoc-testing/, and especially http://jeffreysambells.com/2010/06/22/ios-wireless-app-distribution) but I wanted to put all the details in one place updated for the latest changes to XCode Menu names and using Github Releases. Much of this information can also be found in Apple's Documentation. This link may be useful: https://developer.apple.com/library/ios/documentation/IDEs/Conceptual/AppDistributionGuide/TestingYouriOSApp/TestingYouriOSApp.html The basis tasks are: Create an adhoc profile on the Apple developer site including each beta testers device id. This creates a .mobileprovision file. Create an Archive for your application. This creates a plist file and an .ipa file. Post all three files on the internet (or an intranet). You can use Github Releases. Create a simple HTML file (see this link for the original source of this idea http://jeffreysambells.com/2010/06/22/ios-wireless-app-distribution) that the users will navigate to on their device. The HTML file will contain a link that will allow them to install the latest version of your app. Now the details: Step 1, Hang out on the Apple Developer site for a while I won't pretend to have a solid fundamental understanding of Apple's certificate/provisioning/profile setup/mess, but for this task you definitely need to create an Adhoc Provisioning Profile. This falls into the Distribution category of profiles (as opposed to Development). I am going to list what worked for me circa November 2013. Go to the \"Certificates, Identifiers & Profiles\" section of the Apple Developer site. You need a Distribution Certificate for this task. Create one under \"Certificates -> Production\". Register all your beta tester's iOS devices in the \"Devices\" section. You will need each device's UDID. You can get this by plugging the device into your Mac and opening up the Organizer in XCode (intuitively hidden under \"Windows -> Organizer\"). You need an App ID. I believe since your app is not actually going into the App Store yet you can get away with using a wild card App ID. Create this under the \"Identifiers\" section of the website. Create your Adhoc Provisioning Profile under \"Provisioning Profiles -> Distribution\". You will need to associate your Distribution Certificate, App ID, and each beta tester's device with the profile. Unfortunately this does mean you will need to generate a new one of these each time you want to add a new beta tester. I guess this is one of the reasons this method is free. You will now have a file with the .mobileprovision extension. Step 3 Archive your App In XCode, make sure you have \"iOS device\" selected in the toolbar at the top (the dropdown where you choose which device to run your App on, or which simulator). Click \"Product -> Archive\". Once that succeeds, follow the instructions in step 2 thru 6 here: http://readwrite.com/2010/12/15/apple-best-kept-secret-how-to-do-ad-hoc-installs#awesm=~onnHAUeZCe9nws. You will need to enter the URL where you intend to place the bundle for your users to download. This is because to install your application on their device, users will actually download a .plist file (created in this step), that links to the actual location of your app. As I go into detail below, I put these files on Github Releases, so the location would be something like https : //github.com/<username>/<repo-name>/releases/download/<release-tag-name>/YourApp.ipa You will get a .plist file and a .ipa file. Step 4 & 5 Put these files somewhere your users can access them and create an HTML page that links to your plist You can put these files anywhere you want, as long as people will be able to download them. Thanks to a suggestion from mitalia , I put these on our internal Github using the Releases functionality. You can also use public Github. You just create a release, and attach all three files as binary attachments and Github will store them for you. You can just post the .mobileprovision file directly on Github releases. A word of caution, I have no idea what the ramifications are of posting a provisioning profile on the public internet are, it might be advisable to make the Github repository private, or post this file elsewhere. The link that Github Releases automatically generates for files you bundle with your release will work. Just tell your users to click the link on their iOS device, and it will prompt them to install the provisioning profile on the device. Unfortunately, for installing the app itself you need a little more control over the HTML anchor element, so you need to create an HTML file that contains the properly formatted link. http://jeffreysambells.com/2010/06/22/ios-wireless-app-distribution provides a template HTML index file you can use (and a nice PHP script that makes this simpler if you are not using Github Releases), but effectively you just need to create an HTML page with a link constructed as follows that your users will click on: <a href= \"itms-services://?action=download-manifest&url=LINK_TO_YOUR_PLIST_FILE\" > Click to install </a> Again, you can post this HTML file anywhere. I actually just upload it to Github Releases the same way I upload the other files. The users will see a link to the HTML file in the attachments portion of the Release. In a normal browser clicking the link would result in downloading the file to the machine because Github Release sets the link as \"nofollow\", but in the file system averse iOS, Safari will render the HTML as if the user just clicked on a normal link. They then click the link to your PLIST and will be prompted to install your app. You can then create a new release or update this one as you need users to update the app. They will only need to install the provisioning profile once.","tags":"ios","loc":"http://jeffmax.io/distributing-ios-app-updates-to-beta-testers.html","title":"Distributing iOS App Updates to Beta Testers"},{"text":"The goal of this post is to help get you running with a simple but real Scala project inside an IDE. By this, I mean a small project that has external JAR dependencies. JVM languages typically need some kind of build/dependency tool to get yourself up and running because of the JAR ecosystem. Sometimes just getting the project working with a build tool is enough . With Scala, I decided to use an IDE to help get acquainted with the language. I carefully chronicled all the headaches I ran into while getting up and running and this post will describe those issues and hopefully help ease the pain for others. These instructions worked on OS X. I am not an expert on Ant, Maven, Buildr, or SBT but my experience with using an IDE has taught me that you want to setup a real build tool/dependency manager even if you use an IDE. An alternative option to what I describe here is to have your IDE to create proper Maven/Ant files for you, but I suspect that is pretty complex too. If you let the IDE manage dependencies for you by using the proprietary \"project\" format provided by the IDE you could create problems when you commit to version control. Others will pull down your project and have some or all of the following problems: If you didn't commit your dependent JARS, they will have to find them on their own and then add them as dependencies. If they don't use the same IDE you used they will be completely out of luck. Even if they do use the same IDE you used, it is bound to have directory references to your machine in the project property files that will not work for them. So, we need an IDE and a build system. IDE I have chosen to go with IntelliJ. Read the rest of the section for why, or just skip to the next section. I have used Eclipse with Scala before, but it has always been buggy. I've really wanted to go with Eclipse for Scala because it is backed by Typesafe, but in my opinion it is still not ready. The last version I tried was the one announced here . It did not work well for me, with the following problems. Eclipse froze when I built my first Scala project. Everything was there when I restarted, but that is irritating. This happened with the latest version and the previous version on two separate computers. Import statements in the Worksheet was not working properly for me, and you still need to wrap code in an object to make it run in the Worksheet. I was getting ghost red-x compile errors on in the left margin for lines where there was no code. No support for plain Scala scripts, everything must be in an object or class. IntelliJ did not have any of those problems. Build Tool For Scala, SBT is popular. The best thing you can probably do for yourself is to read the SBT docs before you start. Otherwise there will be a lot of guess work as you blindly copy code from the internet. My biggest problem with the SBT documentation is that it uses a lot of Scala class parameter syntax to describe the concepts. This can be overwhelming for someone learning SBT before they learn Scala. Still, it is worth powering through. One minor, but key point: Typing a command after typing sbt on the command line is not the same as entering the SBT interactive REPL (by just typing sbt) and entering commands. This was a major source of confusion for me in the beginning. For example, to inspect a setting, you do not type sbt inspect < setting > You have to enter the REPL and type inspect < setting > Get Started Install SBT. Instructions are here Once you install SBT, do the following to save yourself some future pain. Set you permgen space higher for the JVM (this has something to do with space reserved for a tier of memory related to garbage collection). Create a file called .sbtconfig in your home directory and put this in it. SBT_OPTS = \"-XX:+CMSClassUnloadingEnabled -XX:MaxPermSize=256M -Xmx512m -Xss2m\" Install the sbt-idea SBT plugin. The instructions are on the page. This will allow SBT to create IntelliJ projects that are a reflection your SBT project configuration. That sounded confusing, but just go with it for now. SBT needs the version of Scala it was compiled with to work, this is different from the version of Scala it will use to compile your projects. This can lead to confusion, so it helps to keep it in mind. The gen-idea plugin works nicely, but currently for some reason it adds the SBT Scala version as a dependency for your projects. This is wrong. Your project is only dependent on the version of Scala you are using for your project. To disable this, create a file in your home directory called .sbtrc and place the following line in it: alias gen - idea = gen - idea no - sbt - build - module Install IntelliJ Open IntelliJ, choose \"Configure\" from the Quickstart menu that pops up. Then choose plugins and search for and install the Scala and SBT plugins. I am not exactly sure you need the SBT plugin, but it doesn't hurt. I believe it enables IntelliJ to use SBT to build your project, but it does not create the required IntelliJ config based off your SBT configuration (that is what the gen-idea SBT plugin does). You can read the documentation for the SBT plugin here . Create your project After all that we are now ready to create an actual project. Here is the main idea: You create a directory and a simple file to tell SBT what version of Scala you want your project to use. You then use SBT and the gen-idea plugin to create an IntelliJ project. You can then open this project in IntelliJ and everything will work. What is the difference between doing all this vs. just creating a Scala project in IntelliJ? IntelliJ requires that you tell it per project where your Scala home directory is. This is annoying. We use the SBT configuration to drive the IntelliJ configuration. You can just commit your SBT configuration files to version control, and others can generate the IntelliJ configuration if they wish to. The sbt-idea plugin can be run multiple times to sync up any changes you made to your SBT configuration with the IntelliJ configuration. For example, if you use SBT to add a jar dependency, you can run the plugin to add this JAR as an IntelliJ dependency (read: generate a bunch of XML files for IntelliJ). Okay. Let's get started. Create a project directory wherever you want and descend into it. Create a file called build.sbt. Put the following in it where is the Scala version you want to use. Right now, I am using 2.10.1. scalaVersion := \" \" If you do not do this, the sbt-idea plugin will create a project that uses whatever version of Scala it was compiled with (probably something like 2.9.2), which is not likely to be what you want. Run the following command: sbt gen-idea Open IntelliJ and then open the project (not import, it is already an IntelliJ project). You're done. You can create a Scala Worksheet and trying something like println ( \"HI\" ) to see that you are in business. Adding JAR dependencies At this point, the most likely next complication will be that you need to use a Java or Scala library that is in a JAR. You could use the IntelliJ mechanism to add an \"External Jar\", but since we have made a point to make the IntelliJ configuration just a reflection of your SBT configuration, the best thing to do would be to add it using SBT and then run the sbt gen - idea command again. There are two main ways to add an SBT dependency to your project- unmanaged and managed . The easiest to do is unmanaged- just create a lib directory in the root of your project directory and drop the JAR you need in there. If you run sbt gen-idea it will tell IntelliJ about it and everything will just work. However, if this is a project to be checked into version control, managed is probably the way to go. Just follow the instructions at the link above and then run sbt gen-idea . One last detail You can tell IntelliJ to build with SBT instead of Make (or whatever its default is). I am not exactly sure what the advantages of this is (especially since you can always do your real build from SBT since the IntelliJ configuration is just a reflection of that), but I have seen situations where the Worksheet functionality did not work properly unless you did this. Follow the instructions here to do that.","tags":"Python","loc":"http://jeffmax.io/creating-a-basic-scala-project-in-an-ide.html","title":"Creating a Basic Scala Project in an IDE\""},{"text":"I love Python, but I have always been a little frustrated by how it handles installing and locating modules. The situation is further complicated by Python Eggs, which I find to be an off-putting name for a endlessly frustrating feature. In particular, I have never really figured out where native support for eggs (just .zip files really) in Python ends and special functionality provided by setuptools begins. In this post I am going to try to figure most of this out in what I hope to be a sane order. As a note, I know a lot of this is made much simpler by just using virtualenv, but these issues still come up. First, a really great introduction to Python modules/packages in general: http://mxm-mad-science.blogspot.com/2008/02/python-eggs-simple-introduction.html He deftly handles a point of confusion I always seem to run into A module and a package are not the same thing. If you are importing a package which would be represented by a directory (on the Python Path) containing a init .py file, you cannot assume that import x x . y will work for every y that this works for from x import y This is because when you import x, you are importing the package x, defined my its init .py file. Unless y is explicitly imported in that file like this: from . import y It will not just work. At least I am not the only one who was confused by this. OK, that is out of the way. Which site-packages will my Python use? It turns out this is a defined at install time, and the defaults vary by system. In particular, look for the details of prefix and exec_prefix at this link http://docs.python.org/install/index.html#how-installation-works . I am still trying to figure out the details, but it also seems that the Python site module is loaded on interpreter startup and can add some system specific locations to the python path. Check this link out for details http://docs.python.org/library/site.html . It's also worth pointing out that Python modules that contain c-extensions are by definition dependent on your processor architecture. On some systems/installers they will therefore be placed in an architecture specific location (probably /usr/lib64 instes of /usr/lib). Holy Moly this is confusing! I rant into a situation today where I installed module with a c-extension, and because a c-extension made the Python module architecture dependent it was installed (by yum on Centos) in the /usr/lib64/python2.7/site-packages directory rather than /usr/lib. However, Python, by default wasn't looking there. Thanks! OK, I found my site-packages directory, where else might it look for modules? .pth files If you have a .pth file in your site-packages directory, the packages and modules (and eggs, yuck!) listed in there will be added to the path. Setup tools puts an easy-install.pth file in there usually, but you can add your own. PYTHONPATH environment variable You can also add locations to the path by adding them to the PYTHONPATH environment variable (just like CLASSPATH in Java). http://www.stereoplex.com/blog/understanding-imports-and-pythonpath eggs This is where I am not even all that sure. I think that natively (without setuptools) Python can support looking in Eggs (which are just zip files) for code. So if you put in egg in site-packages or another directory on the path, or list the egg itself on the path it will work. From the source, http://peak.telecommunity.com/DevCenter/PythonEggs#Using-eggs If you have a pure-Python .egg file that doesn't use any in-package data files, and you don't mind manually placing it on sys.path or PYTHONPATH, you can use the egg without installing setuptools. For eggs containing C extensions, however, or those that need access to non-Python data files contained in the egg, you'll need the pkg_resources module from setuptools installed. So I guess that answers that. I am sure you have run into situations where pkg_resources was missing. It's likely that was caused my a project needing some of the more specialty features of an egg. I've heard, for example , that you some applications will be installable via their setup.py file without setuptools installed; they just won't run without it. Wherein I ramble on about how \"import\" might work with eggs without taking the time actually find out the answer The same site points that you can import an egg (and I assume they mean an egg with with data files or c extensions) with this from pkg_resources import require require ( \"FooBar>=1.2\" ) However this site says: You may use an egg simply by pointing PYTHONPATH or sys.path at it and importing as you normally would, thanks to the import hook changes in recent versions of Python (you need 2.3.5+ or 2.4). If you wish to take this approach, you do not need to bother with setuptool sor ez_setup.py at all. So I am a bit confused as to exactly what happened in 2.3.5 that enables full support for eggs within Python natively. If they are just hooks, do you need to install a library to take advantage of those hooks? Confusing. I will try to figure this out. .egg-info directories Ha, I have no idea what this is. Someone here does though. It's still overly complicated though. Oh, wait! Can we add just one more edge-case here for good measure? (the Python egg cache) Some eggs that are zipped (but not all mind you), for reasons that I have never fully understood (there is an explanation in the note of this stackoverflow answer ), must actually be unzipped and placed somewhere at runtime. By default this is done in /$HOME/.python-eggs. You can override this with the PYTHON_EGG_CACHE environment variable. This is the source of endless problems during deployment when you think everything is working, but then you deploy, and your webserver doesn't have permission to access wherever the egg cache is located. distutils, setuptools, ez_setup.py, easy_install.py, distribute, and distutils2 This is slightly off-topic, but relevant in that it relates to how you might install python modules. The original python module for packaging was called distutils. It is very limited, and PEAK came out with setuptools to enhance its functionality. This introduced the whole egg thing. It included a boostrap utility called ez_setup.py that will install setuptools and and utility called easy_install.py. easy_install.py could then be used from the command line to install python modules from PyPi (the cheeseshop). Setuptools is nice, but the documentation on the PEAK website is infamously terrible, and the project has stagnated. Distribute is a fork of setuptools that add some functionality. To use it Distribute within a project, there is a file called distribute_setup.py that you can package with you code and call from your setup.py that will install it. Then comes distutils2. I did some reading on the web, and the discussion seems to suggest that distutils2 is a re-write of distutils, but done by team that was working on Distribute . Allegedly resources moved from working on Distribute to distutils2, and distutils2 is the future . Interestingly enough, as of the time I wrote this, there seem to be two different repos for distutils2- on bitbucket and python.org . The latest commit on either of those repos was 2 months ago . Distribute, on the other hand, the supposedly less active project, has one repo on bitbucket and the last commit was 11 days ago . So, in short, I have no idea what is going on.","tags":"python","loc":"http://jeffmax.io/python-modules-how-does-python-know-where-to-find-them.html","title":"Python modules: How does Python know where to find them?"},{"text":"A few of my favorite things (in an editor) I have a couple things I really look for in a text editor. 1) I need vertical selection, and I need it to not be really awkward to do. 2) I need to be able to open up a folder as a project. I can't stand when an xml or .property file is required to define a project. The files are in a folder. That's it Making Vim nicer/easier Textmate does both of these, but obviously only works on Mac, and Vim can be made to do both of these things. I have gone back and forth between using Vim and Textmate over the past few years, but I have been using Vim for a while now. A couple of things have contributed to that: MacVim Janus, a well-maintained Vim configuration Some people recommend not using anything like Janus when first getting started with Vim. That may be good advice to some, but I think in general, anything that cuts down on everyone having to go through the same painful (and not strictly necessary) configuration learning curve is probably useful. If you use Janus and really like it, you will probably go back and fill in some of the basics you missed. One of the things I really like with Janus is they have it configured so that out of the box if you type vim . at the command prompt, it will launch vim with Nerdtree (a file folder plugin) showing the current folder. I've never really looked to see if this is complex or easy to do, but with Janus, I don't have to! A slight note: while the above is still useful at times, I have recently been using the ctrlp (or Command-T works too) plugin to quickly search and open the file I want instead of navigating around in the Nerdtree file navigator. Even if you use Janus, you still need to learn how to use Vim as an editor (as opposed to setting up its configuration files) and you will no doubt want to make configuration tweaks. The rest of this post is dedicated to some of my favorite links that explain both Vim configuration and how to actually use it as an editor. Vim Basics This is a good introduction to customizing Vim: http://nvie.com/posts/how-i-boosted-my-vim/ (most of my non-Janus vimrc comes from this one) This is also a introduction to Vim in general: http://wiki.memodrive.com/_media/docs/tools/ vim _tips1.pdf Key Concepts You can use Vim for a while without understanding the concepts introduced in this next post, but this is the only one I have ever seen that does a good job of explaining Vim text objects (as compared to motions). He really breaks down the usage of \"<#> \" in Vim normal mode. http://blog.carbonfive.com/2011/10/17/vim-text-objects-the-definitive-guide/ Once you understand text objects vs motions, this tutorial is a good review. http://mislav.uniqpath.com/2011/12/vim-revisited/ (although he hates Janus) Other enhancements This is a really great plugin, even if you are not using Janus (which just added it). EasyMotion Basically it makes it easy to jump to the exact desired spot on the screen without counting lines and words (who can count anymore anyway?). There are a bunch of settings that I prefer (you can see them here, but I am sure there are better .vimrc files out there ), but there is one that I could not do without: set virtualedit += block This enables you to navigate beyond the end of the line and onto blank lines when in visual block mode. Without this I find it very difficult to use Vim at all, especially when trying to do some of the things I could do in Textmate with vertical selections. Some other great blog posts about Vim http://stevelosh.com/blog/2010/09/coming-home-to-vim/ http://stackoverflow.com/questions/1497958/how-to-use-vim-registers/1498026#1498026 The second most popular answer is really useful and I have never seen it discussed elsewhere.","tags":"Vim","loc":"http://jeffmax.io/yet-another-vim-post.html","title":"Yet Another Vim Post"},{"text":"I have always found the details of the relationships between terminals, standard out/in, and pseudo terminals on Unix variants a bit confusing. This post is my attempt to get some things straight. It may be desultory, over simplified, and at times, blatantly incorrect. Please let me know if I have butchered anything. I will post links as I go along and at the end that may be more enlightening. Please note, I am on OS X, a bsd variant. Terminal Emulator and pseudo terminals First, assuming you are running something like Mac OS X Terminal.app, you are running a terminal emulator. This much is pretty straight-forward. This is an application that emulates in software what used to be a hardware VT-something terminal. You are also running a shell application, possibly bash. Since, in a current day system, both the shell and the Terminal.app are processes running on the machine, rather than the shell outputing to a hardware device, there needs to be some way for the shell to communicate with the Terminal emulator. This is where the Kernel steps in to help out in the form of pseudo-terminals. If, at your terminal, you type: tty The sytstem will respond with something like /dev/ttys001 This is the slave end of your current pseudo-terminal. The slave end is attached to your shell. You also have an associated master side of your pseudo-terminal located at something like: /dev/ptys1 This is attached to the terminal emulator (Terminal.app) From the BSD man pages: \"The slave device provides to a process an interface identical to that described in tty(4). However, whereas all other devices which provide the interface described in tty(4) have a hardware device of some sort behind them, the slave device has, instead, another process manipulating it through the master half of the pseudo terminal. That is, anything written on the master device is given to the slave device as input and anything written on the slave device is presented as input on the master device.\" From Wikipedia: \"Important applications of pseudo terminals include xterm and similar terminal emulators in the X Window System and other window systems like Terminal application in Mac OS X, in which the terminal emulator process is associated with the master device and the shell is associated with the slave. Any terminal operations performed by the shell in a terminal emulator session are received and handled by the terminal emulator process itself (such as terminal resizing or terminal resets). The terminal emulator process receives input from the keyboard and mouse using windowing events, and is thus able to transmit these characters to the shell, giving the shell the appearance of the terminal emulator being an underlying hardware object.\" I believe that /dev/tty is a shortcut to the current slave device (the one returned by the tty command). Officially, it is \" a synonym for the controlling terminal of a process, if any \" (see http://stackoverflow.com/questions/4667154/what-is-the-difference-between-writing-to-stdout-and-a-filehandle-opened-to-dev ). In fact, if you do the following: echo \"HI\" > /dev/tty you should see \"HI\" printed to your terminal. This makes sense because the slave device is connected to the master device, which Terminal.app is attached to. Your bash shell will associate its stdout (file descriptor 1) with the device at /dev/tty. \"Any process it spawns without redirecting stdout will also have this association\" (from http://stackoverflow.com/questions/4667154/what-is-the-difference-between-writing-to-stdout-and-a-filehandle-opened-to-dev ). Additionally, on OS X, /dev/fd/ will contain your file descriptors for the current process, and there are also shortcuts at /dev/stdin, /dev/stdout, and /dev/stderr. Session The session in the container for all the processes that will be associated with a controlling terminal (tty). Each session has one controlling terminal, and vice versa. Session Leader Your shell (or the login process that created the shell) in the above situation would be considered the session leader, because it was the initial process of the session and is interacting with the controlling terminal. From http://www.gnu.org/s/hello/manual/libc/Concepts-of-Job-Control.html \"Usually, new sessions are created by the system login program, and the session leader is the process running the user's login shell.\" Process Groups/Jobs When you execute something from the shell, this is called a command. The command may actually launch more than one process, for example, if you use the pipe (|) to string together a few unix commands. These processes will all belong to the same process group (also called a Job). Process groups are actually what receive signals, not individual processes. All process groups launched from the session leader are in the same session. When the process gets created, it will actually be in the same process group as its parent as well, but as part of standard operating procedure, the shell will immediately set the process to its own unique process group. Processes all inherit the same controlling terminal, stdin and stdout of the session leader (and are called child processes). These process groups are job controlled by the shell (the shell can start, stop, and suspend, background and foreground them). See http://www.gnu.org/s/hello/manual/libc/Concepts-of-Job-Control.html for a more indepth explanation of the Session->Process Group->Process relationship. The shell (session leader) is in charge of job control. It must decide which of the process groups it has started will have access to the controlling terminal. This is accomplished by setting the \"foreground process-group of the controlling terminal\". The reason this distinction must be made is that it needs to be clear which process group should be receiving input from the keyboard and to a lesser extent, which should be allowed to print to the terminal ( I say to a lesser extent because I think on most systems, by default, more than one process group can try to write to the controlling terminal, and it will interleave the output). As an example, if you run ps -la you will see all the processes running (that are associated with a controlling terminal, I believe GUI processes on systems like OS X don't really play by these rules, so you won't see them). If you look in the STAT column you will see some letters. Capital S means the process is sleeping, R means it is running. The second character signifies additional state. s means the process is the session leader, and a + means the process is currently in the foreground process group of its controlling terminal. If you are only running one Terminal, you should see that there is a login process that is the session leader, that the bash process is sleeping, and that the ps process is running and in the foreground process group(R+). You can see in the TTY column what the controlling terminal is for each process, and in this case they will all be the same. This is the same device that would be returned by the \"tty\" command. Just for fun, if you open another terminal, and type ps -la, and look at the bash process on the controlling terminal of your previous terminal, you will see that is now has a + sign associated with it, since the ps process has ended, and the shell itself is now in the foreground process of the controlling terminal again. So, as you are probably aware, running a process with an & after it from the shell will put that process in the background, and return control of the terminal to the shell. The process can probably still write to the terminal, which might cause output to be annoyingly interleaved with what you are trying to do on the shell, but if it tries to read from its stdin, which is the controlling terminal, it will receive a SIGTTIN signal and will be stopped. See these links: http://www.gnu.org/s/hello/manual/libc/Concepts-of-Job-Control.html http://www.gnu.org/s/hello/manual/libc/Access-to-the-Terminal.html#Access-to-the-Terminal http://pubs.opengroup.org/onlinepubs/009604499/basedefs/xbd_chap11.html Creating daemons With this information in hand, we can understand what it takes to create a process that will not die once we log off our shell. First, we need to redirect its standard in, standard out, and standard error to a place that will exist once we log off, because the controlling terminal will be gone. Often this is either a log file or /dev/null. When we log out, the controlling terminal will be destroyed, and the shell will receive the SIGHUP signal, which it then sends to all its child processes (type \"jobs\" at the command line to see who would receive SIGHUP if your terminal closed, these are called processes that are \"under job control\" of the session leader). To prevent our daemon process from receiving this signal, you can either initially execute it wrapped in the \"nohup\" command, or if you decide you need to do it after the fact, you can use the \"disown\" command. Additional Links http://en.wikipedia.org/wiki/Zombie_process http://blog.nelhage.com/tag/termios/ http://rachid.koucha.free.fr/tech_corner/pty_pdip.html http://www.linusakesson.net/programming/tty/index.php https://github.com/nelhage/reptyr","tags":"bash","loc":"http://jeffmax.io/tty-session-leader-terminal-controlling-terminal-stdout-and-stdin-foreground-and-background.html","title":"tty, session leader, terminal, controlling terminal, stdout and stdin, foreground and background"},{"text":"A note on this post: I will be discussing the details to the solution of a problem that was solved by a co-worker of mine. See his blog, The Devel . SQL has always been a bit of a mystery to me. To date, my most complicated queries have been fairly simple table joins. Recently I stumbled upon a problem that I would have traditionally solved by doing two or three separate dynamically generated queries executed from a programming language like Python. My mercifully patient co-workers explained the problem could be solved with one query that uses a few more advanced SQL concepts, including: a subquery GROUP BY HAVING ORDER BY I can see versions of this problem (described below) coming up in other domains. I am going to walk through the solution in this post, from the most naive implementation up through a single query that solves the entire problem. Hopefully it will be a relatable guide to using some of SQL's interesting features. I am also posting a sample SQLite database that you can download and play with while working through the post. If you are not familiar with SQLite, it is a simple, server-less SQL database. It is very widely used and often embedded within other products (it might be running in your web browser). It is perfect for learning SQL because there is virtually no setup and it is almost impossible to break anything. To get started, download the SQLite executable and the movies.db file. Once SQLite is installed, if you open a console and navigate to the location of your downloaded movies.db file and type: sqlite3 movies . db you will be at a SQL prompt ready to go. Slightly Contrived Problem You are in charge of assigning movies to movie reviewers. You have a list of movies, and a list of reviews tagged with the reviewer. When a reviewer asks for an assignment, you must assign him or her a movie that: They have not yet reviewed. Has only been reviewed by at most N other reviewers. Of the movies that fit criteria 1 and 2, each movie is just as likely to be assigned. In summary, if we take N to be 1, you just want every movie to be reviewed by 1 reviewer, and you want every movie to have an equal chance of being reviewed. For the purpose of this post, your data is stored in a database with the following two tables: Movies Table id name length year 1 The Shawshank Redemption 2.5 1994 2 The Dark Knight 3 2008 3 Fight Club 2 1999 4 Goodfellas 2 1990 5 Casablanca 2 1942 Reviews Table id reviewer rating movie 1 Grumpy 0 4 2 Happy 5 2 3 Sleepy ? 3 4 Martin 5 4 How I would solve this with brute-force, mediocre SQL Sleepy wants his next assignment. The simplest solution to the problem is to: Do a select on the reviews table for all reviews by Sleepy. Do a select on the movies table, selecting all movies whose ids are not in the list created in step 1. Iterate through the list of movies from step 2, querying the review table for reviews for that movie. Then choose (at random) a movie whose number of reviews is < N. Lets work through this at the SQLite prompt: sqlite > SELECT * FROM reviews WHERE reviewer = 'Sleepy' ; 3 | Sleepy |?| 3 Then generate a SQL query on the movies table to grab all the movies Sleepy has not yet reviewed. sqlite > SELECT * FROM movies WHERE id NOT IN ( 3 ); 1 | The Shawshank Redemption | 2 . 5 hours | 1994 2 | The Dark Knight | 3 hours | 2008 4 | Goodfellas | 2 hours | 1990 5 | Casablanca | 2 hours | 1942 Finally, iterate through each movie returned in the previous query, constructing a query to count the number of reviews for that movie. sqlite > SELECT COUNT ( * ) FROM reviews WHERE movie = 1 ; 0 sqlite > SELECT COUNT ( * ) FROM reviews WHERE movie = 2 ; 1 sqlite > SELECT COUNT ( * ) FROM reviews WHERE movie = 4 ; 2 sqlite > SELECT COUNT ( * ) FROM reviews WHERE movie = 5 ; 0 Shawshank had no reviews, The Dark Knight had 1 review, Goodfellas had 2 reviews, and Casablanca had no reviews. If N was equal to 1, randomly choose between Shawshank and Casablanca, assigning one of them to Sleepy. All done! How can this be done better? The first step in improving this process is to introduce a subquery. We can essentially do our first two queries above in one call to the database like this: sqlite > SELECT id , name FROM movies WHERE id NOT IN ( SELECT movie FROM reviews WHERE reviewer = 'Sleepy' ); 1 | The Shawshank Redemption 2 | The Dark Knight 4 | Goodfellas 5 | Casablanca We now know the names of the movies that Sleepy has not reviewed. We could now just do step 3 above, iterating through each movie to find out how many reviews we have for each one. Brief Aside One thing I found confusing when going from trivial SQL queries to more complex problems was that by doing a SELECT from more than one table using a WHERE clause, you are doing an implicit inner join. There are quite a few different types of joins that you can actually perform. I have found the following two links helpful: Jeff Atwood explains SQL joins using Venn Diagrams and Wikipedia takes a crack at SQL join types So for example, if we wanted to do an inner join on our two tables, one way would be with the following query: sqlite > SELECT * FROM movies , reviews WHERE movies . id = reviews . movie ; 4 | Goodfellas | 2 hours | 1990 | 1 | Grumpy | 0 | 4 2 | The Dark Knight | 3 hours | 2008 | 2 | Happy | 5 | 2 3 | Fight Club | 2 hours | 1999 | 3 | Sleepy |?| 3 4 | Goodfellas | 2 hours | 1990 | 4 | Martin | 5 | 4 Here we get a row for each movie review. If there is no review, the movie is not showing up at all. This is because it did an implicit inner join. Other types of joins would produce different results. The same query using the more explicit syntax: SELECT * FROM movies INNER JOIN reviews ON ( movies . id = reviews . movie ); 4 | Goodfellas | 2 hours | 1990 | 1 | Grumpy | 0 | 4 2 | The Dark Knight | 3 hours | 2008 | 2 | Happy | 5 | 2 3 | Fight Club | 2 hours | 1999 | 3 | Sleepy |?| 3 4 | Goodfellas | 2 hours | 1990 | 4 | Martin | 5 | 4 For clarity's sake, I will use the more explicit syntax from here on out. But we can do better Let's join these two tables. So if we join these two tables on the movie id, and select only the fields we actually care about, we could write the following query: sqlite > SELECT movies . name , reviews . reviewer FROM movies INNER JOIN reviews ON ( movies . id = reviews . movie ); Goodfellas | Grumpy The Dark Knight | Happy Fight Club | Sleepy Goodfellas | Martin The above query might also be written like this SELECT M . name , R . reviewer FROM movies M INNER JOIN reviews R ON ( M . id = R . movie ); Just look this over for a second and it should become clear that the M and R following the table names are aliases. Everywhere else in the query that alias is used to refer to its associated table name. You will probably see this used in the SQL wild. So we have joined our two tables and grabbed only the information we really cared about. If we add back our subquery, we get this: sqlite > SELECT M . name , R . reviewer FROM movies M INNER JOIN reviews R ON ( M . id = R . movie ) WHERE M . id NOT IN ( SELECT movie FROM reviews WHERE reviews . reviewer = 'Sleepy' ); Goodfellas | Grumpy The Dark Knight | Happy Goodfellas | Martin We now have one row for each movie not reviewed by Sleepy. We could almost solve this problem now without hitting the database again except for the fact that we don't have any information about movies that have no reviews. From here we could do a SELECT * on the movies table, and we would have enough information to programatically figure out what movie to assign Sleepy. But we don't have to hit the database twice We can fix this by changing our join type. If you refer to the links above on join types, you will see there is a \"left outer join\". This will do a join on the tables on the value specified, but even if there is no matching table on the right side of the join, we will still get a row of data from the left table. Lets try it sqlite > SELECT M . name , R . reviewer FROM movies M INNER JOIN reviews R ON ( M . id = R . movie ) WHERE M . id NOT IN ( SELECT movie FROM reviews WHERE reviews . reviewer = 'Sleepy' ); sqlite > SELECT M . name , R . reviewer FROM movies M LEFT OUTER JOIN reviews R ON ( M . id = R . movie ) WHERE M . id NOT IN ( SELECT movie FROM reviews WHERE reviews . reviewer = 'Sleepy' ) ; The Shawshank Redemption | The Dark Knight | Happy Goodfellas | Grumpy Goodfellas | Martin Casablanca | The results are exactly the same as above except that we have a row for Shawshank and Casablanca with no corresponding reviewer. We now have enough information to programatically find a movie for Sleepy to review. Can we make the database do that too? Yes We can use COUNT, GROUP BY, HAVING, ORDER BY, and a database dependent RANDOM() function to have the database return just the name of a movie Sleepy should review. Lets throw COUNT and GROUP BY in there. sqlite > SELECT M . name , R . reviewer FROM movies M INNER JOIN reviews R ON ( M . id = R . movie ) WHERE M . id NOT IN ( SELECT movie FROM reviews WHERE reviews . reviewer = 'Sleepy' ); SELECT M . name , COUNT ( R . reviewer ) FROM movies M LEFT OUTER JOIN reviews R ON ( M . id = R . movie ) WHERE M . id NOT IN ( SELECT movie FROM reviews WHERE reviews . reviewer = 'Sleepy' ) GROUP BY M . name ; Casablanca | 0 Goodfellas | 2 The Dark Knight | 1 The Shawshank Redemption | 0 GROUP BY works by rolling up all identical values of fields after your SELECT statement, aside from variables inside aggregate functions (which COUNT is), into a single row. So instead of getting one row per movie review as before, we are getting one row per unique movie, and the corresponding reviewer variable has been converted into a count of the number of reviewers associated with each row that was grouped. GROUP BY can be confusing, in particular because depending on which fields you put after your SELECT statement and your GROUP BY clause, the results might not make any sense. The general rule is that any field coming after your SELECT statement that is not in an aggregate function (a function that takes many values and outputs one, like COUNT and SUM) must be in your GROUP BY clause as well, otherwise it either will not work, or the meaning of the query is not well defined. GROUP BY really could be its own post (and I am sure it is elsewhere). But we still don't have the database answering this question for us. Lets move that COUNT variable into a HAVING clause. sqlite > SELECT M . name FROM movies M LEFT OUTER JOIN reviews R ON ( M . id = R . movie ) WHERE M . id NOT IN ( SELECT movie FROM reviews WHERE reviews . reviewer = 'Sleepy' ) GROUP BY M . name HAVING COUNT ( R . reviewer ) < 1 ; Casablanca The Shawshank Redemption Here, moving the COUNT function into the HAVING portion of our query, we are able to look at our groups before passing it up to the rest of the query. We are saying: \"only create this group, if the corresponding reviewer count is less than one\". As you can see, we now just have a list of movie names that no one, including Sleepy, has reviewed. It appears that in some databases you cannot refer to a field in your HAVING clause if it was not listed after your SELECT statement, so you might have to do the following (which gives me the chance to introduce the aliasing of fields in your SELECT statement using AS) sqlite > SELECT M . name , count ( R . reviewer ) AS C FROM movies M LEFT OUTER JOIN reviews R ON ( M . id = R . movie ) WHERE M . id NOT IN ( SELECT movie FROM reviews WHERE reviews . reviewer = 'Sleepy' ) GROUP BY M . name HAVING C < 1 ; Here we put the COUNT function back into the select statement, alias it as C, and then refer to that in our HAVING clause. The alias here is optional, I just did it because it can be done, you can alias any field after your SELECT statement, not just functions. It is also important to note that the HAVING portion works on things other than functions. Depending on what you are trying to accomplish it may become confusing as to whether something belongs in a HAVING clause or a WHERE clause. It comes down to where you want to perform the check (either as each row is examined, or as each group is created). I found the following link helpful: http://www.databasejournal.com/features/mysql/article.php/3469351/The-HAVING-and-GROUP-BY-SQL-clauses.htm For this problem though, we definitely need the check to be in the HAVING clause, because the value of the COUNT aggregate function will not yet be defined in the WHERE clause. At the point in time the WHERE clause is being executed the database is still determining which rows are actually in our results and correspondingly, which rows the GROUP BY will be performed on. The HAVING clause allows us to perform checks on the grouped rows. Make the database choose one row at random So we have a query returning a list of movies that Sleepy can review. But he is not a workaholic- he only wants one job, not two. Additionally, he needs to have an equal chance of being assigned either film. We solve the too many jobs problem by sticking a LIMIT on the end of the query: sqlite > SELECT M . name , count ( R . reviewer ) AS C FROM movies M LEFT OUTER JOIN reviews R ON ( M . id = R . movie ) WHERE M . id NOT IN ( SELECT movie FROM reviews WHERE reviews . reviewer = 'Sleepy' ) GROUP BY M . name HAVING C < 1 LIMIT 1 ; Casablanca | 0 This tells the database to only return 1 row from the result set. Unfortunately, every time we run this it will return the same row. We need to get it to randomly pick one of them for us. We can do this by using ORDER BY and then a database specific function. ORDER BY tells the database to return the results ordered by one of the fields. For example, we could ask it to ORDER BY movie.name, and it would always return them in alphabetical order. By using a special function (called RANDOM in SQLite and Postgres), we can have the database assign a random value to each row and then order by that value. So to bring it all together: sqlite > SELECT M . name , count ( R . reviewer ) AS C FROM movies M LEFT OUTER JOIN reviews R ON ( M . id = R . movie ) WHERE M . id NOT IN ( SELECT movie FROM reviews WHERE reviews . reviewer = 'Sleepy' ) GROUP BY M . name HAVING C < 1 ORDER BY RANDOM () LIMIT 1 ; The Shawshank Redemption | 0 If you run this multiple times the result will change. That's it! See this link for database specific random functions: http://www.petefreitag.com/item/466.cfm Any corrections or alternative solutions?","tags":"ios","loc":"http://jeffmax.io/progressively-improving-a-sql-query.html","title":"Progressively Improving a SQL Query"},{"text":"Recently I wanted to make a simple Java servlet without dealing with Eclipse (or any IDE). I have traditionally always used Eclipse to start a new Java project, and then committed whatever mess of xml and .project files the thing dumped onto my computer. This is not fun later. I decided to use Apache Buildr , a Java build framework which appears to be Rake mixed together with Maven. The documentation looks promising, but it is not yet popular enough that google won't try to correct your spelling when you try searching about it. It uses Ruby's syntax, so the build file is not XML like Ant or Maven, but it is compatible with Maven2 respositories. This means there is the chance of finding all your dependencies automatically. Not a good chance though. My project had the following requirements: 1) Build against a jar from a maven repo. 2) Build against some local jars. 3) Remove at least one of the jars from the final war. This had to do with not delivering the servlet-api jar that conflicts with the one tomcat provides. 4) Include an xml file in a specific, non-standard location in the final war. First the build directory structure: buildfile <-- this is the buildr build file lib / <-- folder containing all dependent jars src / main / src / main / webapp / WEB - INF / web.xml <-- war file ' s web.xml src / main / resources / <-- various configuration files src / main / java / <-- Your java code src / main / conf / <-- Non - standard location for resource file The buildr documentation explains that the /src/main/webapp will become the root of the war, your java files will be compiled and placed at /WEB-INF/classes, and the dependent jars will be put into WEB-INF/libs. Now the buildfile: # Generated by Buildr 1.4.5, change to your liking # Version number for this release VERSION_NUMBER = \"1.0.0\" #Group identifier for your projects GROUP = \"SomeWar\" COPYRIGHT = \"\" # Specify Maven 2.0 remote repositories here repositories . remote << \"http://www.ibiblio.org/maven2/\" repositories . remote << \"http://repo1.maven.org/maven2/\" # Specifying an artifact here, Maven style SERVLET = \"javax.servlet:servlet-api:jar:2.4\" desc \"The SomeWar Project\" define \"somewar\" do project . version = VERSION_NUMBER project . group = GROUP manifest [ \"Implementation-Vendor\" ] = COPYRIGHT # Add all the jars in your lib directory to the dependencies compile . with Dir [ _ ( \"lib/*.jar\" )] # Go find that servlet artifact in one of those Maven repos previously specified compile . with SERVLET # Uncomment the line below to show what jars will be included in your war # puts package(:war).libs.map() # This final line says to include my.xml file at # src/main/conf/ as new.xml # at WEB-INF/classes/conf, # and remove the jar specified by the SERVLET artifact package ( : war ). include ( \"src/main/conf/my.xml\" , : as => ' WEB - INF / classes / conf / new . xml ' ). libs -= artifacts ( SERVLET ) end That's it.","tags":"Java","loc":"http://jeffmax.io/create-a-war-file-without-resorting-to-using-eclipse.html","title":"Create a WAR file without resorting to using Eclipse"}]}